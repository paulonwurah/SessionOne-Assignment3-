1.	GIVE A BRIEF DISCRIPTION OF HADOOP ECOSYSTEM: 
Hadoop is a framework that stores and processes data that the normal RDMS cannot. Hadoop in itself is an ecosystem which has two key important components within its ecosystem. They include the storage of data (Hadoop Distributed File System, or HDFS) and the framework for running parallel computations on the already stored data (MapReduce). To get data into the cluster from either a data base or a web application, there are injection tools that are used to perform that task. These injection tools are tools such as flume and sqoop. Respectively, flume is for getting data from web applications into the cluster while sqoop is for getting data from databases into the cluster. To do this, the device which is responsible for communicating with the cluster firstly asks the name node for the locations of the data nodes where the distributed data will be stored. The name node gives the information and with that information, it divides that data into aggregates and injects them to the name node from either a database or web application.
Now, as the data is now into the cluster, what is required is processing whereby we need map reduce.
Map reduce is a software framework for easily writing applications which process vast amounts of data (multi-terabyte data-sets) in-parallel on large clusters (thousands of nodes) of commodity hardware in a reliable, fault-tolerant manner. A map reduce program is written in the device which orchestrates the process of processing the data and then sent to a job tracker. A job tracker is a service in Hadoop that fans map reduce programs to individual task trackers. A task tracker is a node in Hadoop that accepts tasks (fanned out map reduce programs). A map reduce program can either be written in java, pig or hive. When the job is done, the task tracker retrieves the output to the job tracker and then the job tracker to the output folder. This is a brief description of Hadoop ecosystem and its interworking. 
2.	WHAT IS MEANT BY DISTRIBUTED COMPUTING AND WHAT ARE THE DIFFERENT TYPES OF SCALING AVAILABLE? 
Distributed computing is a phenomenon in which components located on series of networked computers communicate and coordinate their actions by passing messages. The components interact with each other in order to achieve a common goal. For example, Hadoop clusters are in a distributed manner whereby they all communicate and interact with each other and acts as one just to achieve a common goal (store and process huge chunk of data). The different types of scaling available include scaling out and scaling up. Scaling up is when you keep on adding disk or same component to a device just to increase its memory while scaling out is when you add disk or same component but in a distributed manner to different devices that are connected with a network so they can act as one, communicate just to achieve a common goal.
3.	WHAT IS MEANT BY COMMODITY HARDWARE IN REAL TIME AND WHAT ARE ITS ADVANTAGES WITH REFERENCE TO ENTERPRISE? 
Commodity hardware is a computer hardware that is affordable and easy to obtain. Typically it is a low-performance system that is IBM PC-compatible and is capable of running Microsoft Windows, Linux, or MS-DOS without requiring any special devices or equipment. Commodity hardware include regular laptops, regular pc e.t.c. Its advantages to of commodity hardware as compared to enterprise is that it is of lower cost, it is easier to obtain, when damaged, it can get replaced easily, it is easier to maintain and easier to master.
4.	WHAT IS MEANT BY REAL TIME AND HOW DATA IS COLLECTED IN THE REAL TIME AND WHAT ARE THE OPTIONS FOR ANALYSING STREAMING DATA? 
Real time is the actual time during which something takes place, occurring immediately. The options for analysing streaming data include using libraries in spark and also storm which are specifically designed for analysing streaming data. 
5.	WHAT IS THE DIFFERENCE BETWEEN HDFS BLOCKS AND INPUT SPLITS? 
Input splits are a logical division of your records whereas HDFS blocks are a physical division of the input data. It’s extremely efficient when they’re the same, but in practice it’s never perfectly aligned. Records may cross block boundaries. Hadoop guarantees the processing of all records. A machine processing a particular split may fetch a fragment of a record from a block other than its “main” block and which may reside remotely. The communication cost for fetching a record fragment is inconsequential because it happens relatively rarely.
6.	40mins

N:B- In question 5, it’s a little confusing at the middle as the question how is real time data collected. I’m not too clear with the question. 
